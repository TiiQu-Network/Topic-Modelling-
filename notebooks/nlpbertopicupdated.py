# -*- coding: utf-8 -*-
"""nlpbertopicupdated.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A4Y7egSCjQ71BqeMFIpTf6DNfc-042i5
"""

from google.colab import drive
drive.mount('/content/drive')

import gensim
import numpy as np
import pandas as pd
import scipy
import seaborn as sns
import spacy
import sklearn as sk

"""Import our topic modeling library `BERTopic` and openai modules"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install git+https://github.com/MaartenGr/BERTopic huggingface_hub safetensors -qqq

import bertopic
from umap import UMAP
from hdbscan import HDBSCAN
from sklearn.feature_extraction.text import CountVectorizer
from bertopic.vectorizers import ClassTfidfTransformer
from bertopic.dimensionality import BaseDimensionalityReduction

"""Load dataset"""

dir = '/content/drive/MyDrive/15kdataset.csv'

"""# load data"""

train_dfs = [ pd.read_csv( f'{dir}', encoding='utf8' ) ]
print( train_dfs[ 0 ].shape )
train_dfs[ 0 ].head()

train_dfs = pd.read_csv(dir, encoding='utf8')
print(train_dfs.shape)
train_dfs.head()

"""# preprocess (without lemmatization)

The only steps involved here are to concatentate `Question` and `Answer` and remove footnote numbers
"""

import pandas as pd

# Initialize the train_dfs list
train_dfs = []

# Populate the train_dfs list with DataFrames
train_dfs.append(pd.read_csv(dir))
# Add more files as needed

# Loop through the train_dfs list
for df in train_dfs:
    df['text'] = df.Question + ' ' + df.Answer.str.replace(r'\[[0-9]*\]', '')
    df['text'] = df['text'].astype(str)  # Convert float to string

"""with remove stopwords, speech tagging

"""

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk import pos_tag

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')

# Initialize stopwords
stop_words = set(stopwords.words('english'))

# Initialize the train_dfs list
train_dfs = []

# Populate the train_dfs list with DataFrames
train_dfs.append(pd.read_csv(dir))
# Add more files as needed

# Loop through the train_dfs list
for df in train_dfs:
    df['text'] = df.Question + ' ' + df.Answer.str.replace(r'\[[0-9]*\]', '')
    df['text'] = df['text'].astype(str)  # Convert float to string

    # Tokenize words and remove stopwords
    df['tokenized_text'] = df['text'].apply(lambda x: word_tokenize(x))
    df['filtered_text'] = df['tokenized_text'].apply(lambda x: [word for word in x if word.lower() not in stop_words])

    # Tag parts of speech
    df['pos_tags'] = df['filtered_text'].apply(pos_tag)

train_dfs[0].head()

"""# train topic models

1. **Document embedding**: The sentence transformer `all-MiniLM-L6-v2` is used by default to create a vector embedding for each document. Other supported models can be considered when the quality of topics given by `all-MiniLM-L6-v2` is not good enough
1. **Dimensionality reduction**: The number of dimensions is by defaul 5. A larger number may be considered when it is believed finer granularity is needed to store the information for sentence similarity comparison. The default metric is cosine similarity. However, some points out Euclidean distance could be a better choice as UMAP relies on Euclidean space. [ref](https://github.com/lmcinnes/umap/issues/519)
1. **Document clustering**: The minimum cluster size in HDBSCAN can be tuned but the change is subtle.
1. **Weighing scheme**: enabling `bm25_weighing` and `reduce_frequent_words` may lead to overfitting.
"""

def train( m, df ):
  docs = df.text
  # train topc model
  topics, probs = m.fit_transform( docs )
  return topics, probs

def calculate_coherence_score( m, df ):
  docs = df.text
  # calculate coherence score
  analyzer = m.vectorizer_model.build_analyzer()
  cleaned_docs = m._preprocess_text( docs )
  tokenized_docs = [ analyzer( doc ) for doc in cleaned_docs ]
  dictionary = gensim.corpora.Dictionary( tokenized_docs )
  return gensim.models.CoherenceModel(
    topics=[ [ topic for topic, freq in m.get_topic( i ) if topic in dictionary.token2id ] for i in range( len( set( m.get_topics() ) ) - 1 ) ],
    corpus=[ dictionary.doc2bow( doc ) for doc in tokenized_docs ],
    dictionary=dictionary,
    texts=tokenized_docs,
    coherence='c_v',
  ).get_coherence()

data_type = type(train_dfs[0])
print("Data type of train_dfs[0]:", data_type)

from bertopic import BERTopic
from bertopic.representation import KeyBERTInspired

representation_model = KeyBERTInspired()

m = BERTopic("english", verbose=True, nr_topics=50, representation_model=representation_model)

m = bertopic.BERTopic()
topics, probs = train( m, train_dfs[ 0 ] )
len( m.get_topics() )

calculate_coherence_score( m, train_dfs[ 0 ] )

"""# generate topic names

Topic names are generated based on the top 50 words in the bag of words in each topic
"""

topic_words = np.r_[ [ [ word for word, score in scored_words ] for k, scored_words in m.get_topics().items() ] ]
print( topic_words.shape )
topic_words[ :, 0 ]

def generate_topic_name(words):
    # Create a topic name with at most 3 words from the input list
    topic_name = ' '.join(words[:1])

    return topic_name

import nltk
from nltk.corpus import wordnet

# Download WordNet data (only required once)
nltk.download('wordnet')

def is_noun(word):
    # Check if the word is a noun using WordNet
    synsets = wordnet.synsets(word)
    return any(syn.pos() == wordnet.NOUN for syn in synsets)

def generate_topic_name(words):
    # Filter out non-noun words from the input list
    noun_words = [word for word in words if is_noun(word)]

    # Create a topic name with at most 3 noun words from the input list
    topic_name = ' '.join(noun_words[:1])

    return topic_name

topic_nums = np.r_[ [ k for k in m.get_topics().keys() ] ]

topic_names = { num: generate_topic_name( words ) for [ words, num ] in zip( topic_words[ :, : 50 ], topic_nums ) }

train_dfs[ 0 ][ 'topic_words' ] = [ ', '.join( topic_words[ topic_num ] ) for topic_num in topics ]

print(list(train_dfs[0].keys()))

train_dfs[0][ 'topic_name' ] = [ topic_names[ topic_num ] for topic_num in topics ]

train_dfs[ 0 ][ [ 'Question', 'Answer', 'topic_name', 'topic_words' ] ].to_csv(
    '/content/drive/MyDrive/nvx[[[xcc41.csv',
    index=False,
)

"""# visualization"""

m.visualize_topics()

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def create_wordcloud( model, topic ):
  words = { word: score for word, score in model.get_topic( topic ) }
  wc = WordCloud( max_words=100 )
  wc.generate_from_frequencies( words )
  plt.imshow( wc, interpolation='bilinear' )
  plt.axis( 'off' )
  plt.show()

for i, topic in enumerate( m.get_topics() ):
  if topic >= 0:
    print( f'topic #{topic}' )
    create_wordcloud( m, topic )
  if i > 10:
    break